# 小型Transformer快速优化：15分钟预训练消融实验

[English Version](README.md) | 中文版

亲自运行实验: **[Google Colab](https://colab.research.google.com/drive/1Fn5Dw-8xKKSt915dwNvqLwA7V0Bdw0IY?usp=sharing)** 或 [GitHub](https://github.com/vukrosic/do-small-llm-research/blob/main/15min_llm_ablations.ipynb)

## 概述

本仓库包含对小型自回归Transformer语言模型的全面消融研究，系统评估了在资源受限环境下关键超参数和架构选择对训练效率和性能的影响。我们测试了六种配置：学习率(1e-4到1e-3)、批大小(8到16)、序列长度(256到512)和模型规模(256到384维)。实验使用Tesla T4 GPU在SmolLM语料库上进行混合精度训练，每个配置训练2000步(约15分钟)。

## 🚀 核心发现

- **批大小缩放至关重要**：大批次配置性能提升2.7倍(训练损失1.334 vs 基线3.642)
- **学习率极度敏感**：不良学习率导致困惑度增加36倍(178.18 vs 4.93)
- **存在效率权衡**：小模型训练速度快39%(87.7 vs 63.1步/秒)
- **短时训练揭示洞见**：每个配置仅需15分钟获得有意义结果
- **数据曝光量影响**：大批量配置处理2倍token，但损失改善超2倍

## 📊 实验结果

| 配置方案      | 训练损失 | 准确率 | 困惑度 | 训练速度(步/秒) |
|---------------|----------|--------|--------|----------------|
| 大批量        | **1.334**| **69.8%**| **4.93** | 63.12          |
| 高学习率      | 2.227    | 41.2%  | 18.82   | 60.72          |
| 基线          | 3.642    | 30.4%  | 48.85   | 64.51          |
| 短序列        | 3.875    | 34.9%  | 43.69   | 64.57          |
| 小模型        | 4.053    | 29.4%  | 77.25   | **87.73**      |
| 低学习率      | 4.893    | 25.4%  | 178.18  | 63.25          |

## 🔧 快速开始

通过 **[Google Colab](https://colab.research.google.com/drive/1Fn5Dw-8xKKSt915dwNvqLwA7V0Bdw0IY?usp=sharing)** 运行

若无法访问，可从[GitHub](https://github.com/vukrosic/do-small-llm-research/blob/main/15min_llm_ablations.ipynb)下载

## 📈 测试配置方案

### 1. 基线配置
- **模型**: 384维, 6头, 12层
- **训练**: 批大小8, 学习率3e-4, 序列长512
- **目的**: 标准参考配置

### 2. 高学习率
- **调整**: 学习率1e-3(批大小8, 序列长512)
- **结果**: 快速收敛但不稳定(损失:2.227)
- **洞见**: 激进学习率适合短时训练

### 3. 低学习率
- **调整**: 学习率1e-4(批大小8, 序列长512)
- **结果**: 15分钟内收敛差(损失:4.893)
- **洞见**: 保守学习率不适合快速原型开发

### 4. 大批量
- **调整**: 批大小16, 学习率5e-4(序列长512)
- **结果**: 最佳综合表现(损失:1.334, 准确率69.8%)
- **洞见**: 批大小缩放对优化稳定性关键

### 5. 短序列
- **调整**: 序列长256, 批大小12, 学习率3e-4
- **结果**: 训练更快但性能下降(损失:3.875)
- **洞见**: 上下文长度与效率的权衡

### 6. 小模型
- **调整**: 256维, 4头, 8层, 批大小12, 学习率3e-4
- **结果**: 最快训练速度(87.7步/秒), 中等性能(损失:4.053)
- **洞见**: 适合快速迭代

## 🎯 应用场景

### 研发领域
- **快速原型设计**: 15分钟验证想法
- **超参探索**: 快速迭代周期
- **架构比较**: 高效模型选择

### 教育领域
- **Transformer学习**: 动手实验
- **优化理解**: 可视化训练动态
- **资源受限学习**: 适合所有学生

### 生产环境
- **模型选择**: 选择最优配置
- **资源规划**: 理解计算需求
- **基线建立**: 大型实验的起点

## 📊 深度分析

### 单位数据曝光性能
控制数据曝光量后的重要发现:

| 配置方案      | 总token数(百万) | 损失/token效率 |
|---------------|-----------------|----------------|
| 大批量        | 16.38          | **0.0815**     |
| 高学习率      | 8.19           | 0.2719         |
| 小模型        | 12.29          | 0.3298         |
| 基线          | 8.19           | 0.4446         |
| 短序列        | 6.14           | 0.6309         |
| 低学习率      | 8.19           | 0.5975         |

### 置信度与校准分析
最佳模型显示更优预测置信度:
- **大批量**: 0.502置信度, 最低熵(校准良好)
- **高学习率**: 0.345置信度, 稳定进展
- **低学习率**: 0.217置信度, 最高熵(校准差)

### Top-5准确率结果

| 配置方案      | Top-5准确率 | Top-1差距 | 提升幅度 |
|---------------|-------------|-----------|----------|
| 大批量        | **87.6%**   | 17.8%     | **86.1%**|
| 高学习率      | 65.9%       | 24.7%     | 64.3%    |
| 基线          | 53.6%       | 23.2%     | 52.3%    |
| 短序列        | 55.1%       | 20.2%     | 53.7%    |
| 小模型        | 49.1%       | 19.7%     | 47.4%    |
| 低学习率      | 41.7%       | 16.3%     | 40.1%    |

## 📊 可视化示例

包含全面的可视化分析:

![训练损失对比](research/training_loss_comparison.png)

- **训练损失曲线**: 比较不同配置收敛模式
- **性能指标**: 最终准确率、困惑度对比
  ![最终性能对比](research/final_performance_comparison.png)
- **效率分析**: 步数/秒、训练时间权衡
  ![效率分析](research/efficiency_analysis.png)
- **收敛分析**: 早期与后期动态
  ![收敛分析](research/convergence_analysis.png)

## 🔬 研究方法

### 模型架构
- **类型**: 仅解码器的自回归Transformer
- **注意力**: 带旋转位置嵌入(RoPE)的多头注意力
- **激活函数**: SiLU(Swish)增强梯度流
- **归一化**: RMSNorm保证训练稳定性
- **前馈网络**: 标准FFN，维度可配置

### 数据集
- **来源**: HuggingFaceTB/smollm-corpus(cosmopedia-v2)
- **规模**: 500份文档
- **分词器**: HuggingFaceTB/SmolLM-135M

### 训练设置
- **硬件**: NVIDIA Tesla T4 GPU
- **时长**: 2000步(每个配置约15分钟)
- **评估**: 每400步一次
- **精度**: 自动混合精度(AMP)
- **优化器**: AdamW(权重衰减0.1, β₁=0.9, β₂=0.95)
- **调度器**: 带热身的余弦退火

### 跟踪指标
- 训练/验证损失和准确率
- 困惑度和置信度分数
- Top-5准确率和校准指标
- 训练效率(步数/秒)
- 梯度范数和收敛模式

## ⚠️ 局限性与注意事项

### 方法论局限
- **短时训练**: 仅2000步可能无法反映长期行为
- **单次实验**: 缺乏统计显著性检验
- **未控数据曝光**: 不同批大小处理数据量不同
- **单一数据集**: 仅在SmolLM语料库测试
- **硬件限制**: Tesla T4结果可能不适用于其他架构

### 关键分析
- **数据曝光混杂**: 大批量优势部分源于处理2倍token
- **训练时长偏差**: 可能偏向快速收敛配置
- **硬件特异性**: 结果特定于T4 GPU特性

## 🚀 未来研究方向

### 近期扩展
1. **控制数据曝光**: 固定token预算跨配置
2. **统计验证**: 不同种子的多次运行
3. **延长训练**: 10K+步观察长期收敛
4. **跨数据集验证**: 多领域评估

### 进阶研究
1. **学习率调度**: 热身策略、周期性调整
2. **正则化效应**: Dropout变体、权重衰减缩放
3. **架构变体**: 专家混合、稀疏注意力
4. **缩放定律**: 模型规模、数据和计算的性能缩放
5. **多GPU分析**: 分布式训练模式

### 方法论改进
1. **公平比较协议**: 标准化计算预算评估
2. **不确定性量化**: 贝叶斯方法建模不确定性
3. **硬件泛化**: 跨平台验证

## 📝 引用

如需引用:

```bibtex
@misc{rosic2025transformer_ablation,
  title={小型Transformer语言模型的快速优化：资源受限环境下学习率、批大小和架构效率的15分钟预训练消融研究},
  author={Rosić, Vuk and Claude},
  year={2025},
  note={GitHub托管研究论文},
  howpublished={\url{https://github.com/vukrosic/do-small-llm-research}}
}
```

## 🤝 贡献指南

欢迎贡献！请遵循指南:
1. Fork本仓库
2. 创建特性分支
3. 添加实验或改进
4. 提交包含详细描述的PR

### 重点贡献方向
- 新配置实验
- 额外可视化工具
- 性能优化
- 文档改进

## 📄 许可证

本项目采用MIT许可证 - 详见[LICENSE](LICENSE)文件

## 🙏 致谢

- **HuggingFace**: 提供SmolLM语料库和transformers库
- **PyTorch团队**: 优秀深度学习框架
- **Google Colab**: 提供可用GPU资源
- **研究社区**: 开放科学与可重复研究

## 📞 联系方式

- **作者**: Vuk Rosić
- **邮箱**: vukrosic1@gmail.com
- **机构**: 欧布达大学

---

**⭐ 如果本仓库对您的研究有帮助，请点星支持！**